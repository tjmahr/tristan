---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "fig/README-"
)
```

# tristan [![Project Status: WIP - Initial development is in progress, but there has not yet been a stable, usable release suitable for the public.](http://www.repostatus.org/badges/latest/wip.svg)](http://www.repostatus.org/#wip)

This package contains my helper functions for working with models fit with 
RStanARM. The package is named tristan because I'm working with Stan samples and
my name is Tristan.

I plan to incrementally update this package whenever I find myself solving the
same old problems from an RStanARM model.

## Installation

You can install tristan from github with:

```{r gh-installation, eval = FALSE}
# install.packages("devtools")
devtools::install_github("tjmahr/tristan")
```

## Overview of helpers

`augment_posterior_predict()` and `augment_posterior_linpred()` generate new 
data predictions and fitted means for new datasets using RStanARM's 
`posterior_predict()` and `posterior_linpred()`. The RStanARM functions return 
giant matrices of predicted values, but these functions return a long dataframe
of predicted values along with the values of the predictor variables. The name
_augment_ follows the convention of the broom package where `augment()` refers
to augmenting a data-set with model predictions.

`stan_to_lm()` and `stan_to_glm()` provide a quick way to refit an RStanARM 
model with its classical counterpart.

The `ggs()` function in the ggmcmc package produces a tidy dataframe of MCMC
samples. That function doesn't return the original parameter names for RStanARM
models. `ggs_rstanarm()` rectifies this problem.


## Example

Fit a simple linear model.

```{r model, message = FALSE, results = 'hide'}
library(tidyverse)
library(rstanarm)
library(tristan)

# Scaling makes the model run much faster
scale_v <- function(...) as.vector(scale(...))
iris$z.Sepal.Length <- scale_v(iris$Sepal.Length)
iris$z.Petal.Length <- scale_v(iris$Petal.Length)

# Just to ensure that NA values don't break the prediction function
iris[2:6, "Species"] <- NA

model <- stan_glm(
  z.Sepal.Length ~ z.Petal.Length * Species,
  data = iris,
  family = gaussian(),
  prior = normal(0, 2))
```

```{r}
print(model)
```


### Posterior fitted values (linear predictions)

Let's plot some samples of the model's linear prediction for the mean. If
classical model provide a single "line of best fit", Bayesian models provide a
distribution "lines of plausible fit". We'd like to visualize 100 of these lines
alongside the raw data.

In classical models, getting the fitted values is easily done by adding a column
of `fitted()` values to dataframe or using `predict()` on some new observations.

Because the posterior of this model contains 4000 such fitted or predicted 
values, more data wrangling and reshaping is required. 
`augment_posterior_linpred()` automates this task by producing a long dataframe
with one row per posterior fitted value.

Here, we tell the model that we want just 100 of those lines (i.e., 100 samples
from the posterior distribution).

```{r}
# Get the fitted means of the data for 100 samples of the posterior distribution
linear_preds <- augment_posterior_linpred(
  model = model, 
  newdata = iris, 
  nsamples = 100)
linear_preds
```

To plot the lines, we have to unscale the model's fitted values.

```{r}
unscale <- function(scaled, original) {
  (scaled * sd(original, na.rm = TRUE)) + mean(original, na.rm = TRUE)
}

linear_preds$.posterior_value <- unscale(
  scaled = linear_preds$.posterior_value, 
  original = iris$Sepal.Length)
```

Now, we can do a spaghetti plot of linear predictions. 

```{r many-lines-of-best-fit, fig.height = 4, fig.width = 6}
ggplot(iris) + 
  aes(x = Petal.Length, y = Sepal.Length, color = Species) + 
  geom_point() + 
  geom_line(aes(y = .posterior_value, group = interaction(Species, .draw)), 
            data = linear_preds, alpha = .20)
```

### Posterior predictions (simulated new data)

`augment_posterior_predict()` similarly tidies values from the
`posterior_predict()` function. `posterior_predict()` incorporates the error
terms from the model, so it can be used predict new fake data from the model.

Let's create a range of values within each species, and get posterior 
predictions for those values.

```{r}
library(modelr)

# Within each species, generate a sequence of z.Petal.Length values
newdata <- iris %>% 
  group_by(Species) %>% 
  # Expand the range x value a little bit so that the points do not bulge out
  # left/right sides of the uncertainty ribbon in the plot
  data_grid(z.Petal.Length = z.Petal.Length %>% 
              seq_range(n = 80, expand = .10)) %>% 
  ungroup()

newdata$Petal.Length <- unscale(newdata$z.Petal.Length, iris$Petal.Length)

# Get posterior predictions
posterior_preds <- augment_posterior_predict(model, newdata)
posterior_preds

posterior_preds$.posterior_value <- unscale(
  scaled = posterior_preds$.posterior_value, 
  original = iris$Sepal.Length)
```

Take a second to appreciate the size of that table. It has 4000 predictions for
each the `r nrow(newdata)` observations in `newdata`.

Now, we might inspect whether 95% of the data falls inside the 95% interval of
posterior-predicted values (among other questions we could ask the model.)

```{r 95-percent-intervals, fig.height = 4, fig.width = 6}
ggplot(iris) + 
  aes(x = Petal.Length, y = Sepal.Length, color = Species) + 
  geom_point() + 
  stat_summary(aes(y = .posterior_value, group = Species, color = NULL), 
               data = posterior_preds, alpha = 0.4, fill = "grey60", 
               geom = "ribbon", 
               fun.data = median_hilow, fun.args = list(conf.int = .95))
```

### Refit RStanARM models with classical counterparts

There are some quick functions for refitting RStanARM models using classical 
versions. These functions basically inject the values of `model$formula` and 
`model$data` into `lm()` or `glm()`. (Seriously, see the call sections in the
two outputs below.) Therefore, don't use these functions for serious comparisons
of classical versus Bayesian models.

Refit with a linear model:

```{r}
arm::display(stan_to_lm(model))
```

Refit with a generalized linear model:

```{r}
arm::display(stan_to_glm(model))
```

```{r, echo = FALSE, eval = FALSE}
arm::display(glm( z.Sepal.Length ~ z.Petal.Length * Species,
  data = iris,
  family = gaussian()))
```




### ggmc support

ggmc provides [a lot of 
magic](http://xavier-fim.net/packages/ggmcmc/#importing-mcmc-samples-into-ggmcmc-using-ggs).
The general ggmcmc workflow is to create a tidy dataframe using `ggs()` and plug
it that into the package's plotting functions. For example, here is how we can
inspect the each parameter value using histograms and interval plots.

```{r ggmc-no-name, fig.width=4, fig.height=4, fig.show = "hold"}
library(ggmcmc)
gg_model <- ggs(model)
  
# Facet wrap so that values are in a grid, not a single column.
ggs_histogram(gg_model) + 
  facet_wrap("Parameter", scales = "free_x")

# Remap y-aesthetic so that the parameters read from top-to-bottom in their
# original factor ordering.
ggs_caterpillar(gg_model, line = 0) + 
  aes(y = forcats::fct_rev(Parameter)) + 
  ylab(NULL)
```

The package is magically convenient. But look, the plot lost the names of
parameters from the model!

`ggs_rstanarm()` is a small function that imitates the output of `ggs()` but
tries to keep the original parameter names. It also drops the non-parameter
`"mean_PPD"` (the model's prediction for a completely average observation.)

```{r ggmc-yes-name, fig.width=4, fig.height=4, fig.show = "hold"}
gg_model2 <- ggs_rstanarm(model)

ggs_histogram(gg_model2) + 
  facet_wrap("Parameter", scales = "free_x")

ggs_caterpillar(gg_model2, line = 0) + 
  aes(y = forcats::fct_rev(Parameter)) + 
  ylab(NULL)
```


### Calculate _R_<sup>2</sup>

`calculate_model_r2()` returns the unadjusted _R_<sup>2</sup> for each draw of
the posterior distribution.

```{r}
df_r2 <- data_frame(
  R2 = calculate_model_r2(model)
)
df_r2
```

Now, we can compare the difference between highest-posterior intervals and
equal-tailed intervals.

```{r}
get_interval_edges <- function(prob) {
  tail <- (1 - prob) / 2
  c(tail, 1 - tail)
}

hpd <- coda::HPDinterval(coda::as.mcmc(df_r2$R2), prob = 0.95)
ci95 <- quantile(df_r2$R2, get_interval_edges(.95))

# Different widths
max(hpd) - min(hpd)
max(ci95) - min(ci95)
```

And we can plot the distribution of the _R_<sup>2</sup> values.

```{r r2-histogram, fig.width=6, fig.height=4}
cis <- data_frame(
  Method = c("HPD", "HPD", "Equal Tail", "Equal Tail"),
  Value = c(as.vector(hpd), ci95))


ggplot(df_r2) + 
  aes(x = R2) + 
  geom_histogram() + 
  geom_vline(aes(xintercept = Value, color = Method), 
             data = cis, size = 1, linetype = "dashed") + 
  labs(color = "95% interval",
       x = expression(R^2),
       y = "Num. posterior samples")
```

